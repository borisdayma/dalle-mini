{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "DALL-E Mini Inferencing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/borisdayma/dalle-mini/blob/main/DALL_E_Mini_Inferencing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6d33374"
      },
      "source": [
        "# Inferencing [DALL-E Mini](https://github.com/borisdayma/dalle-mini)\n",
        "\n",
        "### Colab by mega b#6696 | Originally from [here](https://github.com/borisdayma/dalle-mini/tree/main/demo)\n",
        "\n",
        "### Heavily recommended to use a GPU runtime. Though, if needed, CPU works as well."
      ],
      "id": "f6d33374"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eb74941-bb4d-4d7e-97f1-d5a3a07672bf",
        "cellView": "form"
      },
      "source": [
        "#@title # **Setup** run this once.\n",
        "\n",
        "#@markdown ### You will be prompted to enter your wandb.ai authorization\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install flax transformers\n",
        "!git clone https://github.com/patil-suraj/vqgan-jax.git\n",
        "\n",
        "!pip install wandb\n",
        "\n",
        "clear_output()\n",
        "\n",
        "import wandb\n",
        "\n",
        "run = wandb.init()\n",
        "artifact = run.use_artifact('wandb/hf-flax-dalle-mini/model-3iwhu4w6:latest', type='bart_model')\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "clear_output()\n",
        "\n",
        "import random\n",
        "\n",
        "import jax\n",
        "import flax.linen as nn\n",
        "from flax.training.common_utils import shard\n",
        "from flax.jax_utils import replicate, unreplicate\n",
        "\n",
        "from transformers.models.bart.modeling_flax_bart import *\n",
        "from transformers import BartTokenizer, FlaxBartForConditionalGeneration\n",
        "\n",
        "import io\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "jax.devices()\n",
        "\n",
        "%cd /content/vqgan-jax\n",
        "\n",
        "!pip install -e .\n",
        "\n",
        "from vqgan_jax.modeling_flax_vqgan import VQModel\n",
        "%cd /content/\n",
        "\n",
        "# TODO: set those args in a config file\n",
        "OUTPUT_VOCAB_SIZE = 16384 + 1  # encoded image token space + 1 for bos\n",
        "OUTPUT_LENGTH = 256 + 1  # number of encoded tokens + 1 for bos\n",
        "BOS_TOKEN_ID = 16384\n",
        "BASE_MODEL = 'facebook/bart-large-cnn'\n",
        "\n",
        "class CustomFlaxBartModule(FlaxBartModule):\n",
        "    def setup(self):\n",
        "        # we keep shared to easily load pre-trained weights\n",
        "        self.shared = nn.Embed(\n",
        "            self.config.vocab_size,\n",
        "            self.config.d_model,\n",
        "            embedding_init=jax.nn.initializers.normal(self.config.init_std, self.dtype),\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "        # a separate embedding is used for the decoder\n",
        "        self.decoder_embed = nn.Embed(\n",
        "            OUTPUT_VOCAB_SIZE,\n",
        "            self.config.d_model,\n",
        "            embedding_init=jax.nn.initializers.normal(self.config.init_std, self.dtype),\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "        self.encoder = FlaxBartEncoder(self.config, dtype=self.dtype, embed_tokens=self.shared)\n",
        "\n",
        "        # the decoder has a different config\n",
        "        decoder_config = BartConfig(self.config.to_dict())\n",
        "        decoder_config.max_position_embeddings = OUTPUT_LENGTH\n",
        "        decoder_config.vocab_size = OUTPUT_VOCAB_SIZE\n",
        "        self.decoder = FlaxBartDecoder(decoder_config, dtype=self.dtype, embed_tokens=self.decoder_embed)\n",
        "\n",
        "class CustomFlaxBartForConditionalGenerationModule(FlaxBartForConditionalGenerationModule):\n",
        "    def setup(self):\n",
        "        self.model = CustomFlaxBartModule(config=self.config, dtype=self.dtype)\n",
        "        self.lm_head = nn.Dense(\n",
        "            OUTPUT_VOCAB_SIZE,\n",
        "            use_bias=False,\n",
        "            dtype=self.dtype,\n",
        "            kernel_init=jax.nn.initializers.normal(self.config.init_std, self.dtype),\n",
        "        )\n",
        "        self.final_logits_bias = self.param(\"final_logits_bias\", self.bias_init, (1, OUTPUT_VOCAB_SIZE))\n",
        "\n",
        "class CustomFlaxBartForConditionalGeneration(FlaxBartForConditionalGeneration):\n",
        "    module_class = CustomFlaxBartForConditionalGenerationModule\n",
        "\n",
        "# create our model\n",
        "tokenizer = BartTokenizer.from_pretrained(BASE_MODEL)\n",
        "model = CustomFlaxBartForConditionalGeneration.from_pretrained(artifact_dir)\n",
        "model.config.force_bos_token_to_be_generated = False\n",
        "model.config.forced_bos_token_id = None\n",
        "model.config.forced_eos_token_id = None\n",
        "\n",
        "# we verify that the shape has not been modified\n",
        "model.params['final_logits_bias'].shape\n",
        "\n",
        "vqgan = VQModel.from_pretrained(\"flax-community/vqgan_f16_16384\")\n",
        "\n",
        "def custom_to_pil(x):\n",
        "    x = np.clip(x, 0., 1.)\n",
        "    x = (255*x).astype(np.uint8)\n",
        "    x = Image.fromarray(x)\n",
        "    if not x.mode == \"RGB\":\n",
        "        x = x.convert(\"RGB\")\n",
        "    return x\n",
        "\n",
        "def generate(input, rng, params):\n",
        "  return model.generate(\n",
        "      **input,\n",
        "      max_length=257,\n",
        "      num_beams=1,\n",
        "      do_sample=True,\n",
        "      prng_key=rng,\n",
        "      eos_token_id=50000,\n",
        "      pad_token_id=50000,\n",
        "      params=params\n",
        "  )\n",
        "\n",
        "def get_images(indices, params):\n",
        "    return vqgan.decode_code(indices, params=params)\n",
        "\n",
        "\n",
        "def plot_images(images):\n",
        "    fig = plt.figure(figsize=(40, 20))\n",
        "    columns = 4\n",
        "    rows = 2\n",
        "    plt.subplots_adjust(hspace=0, wspace=0)\n",
        "\n",
        "    for i in range(1, columns*rows +1):\n",
        "        fig.add_subplot(rows, columns, i)\n",
        "        plt.imshow(images[i-1])\n",
        "    plt.gca().axes.get_yaxis().set_visible(False)\n",
        "    plt.show()\n",
        "    \n",
        "def stack_reconstructions(images):\n",
        "    w, h = images[0].size[0], images[0].size[1]\n",
        "    img = Image.new(\"RGB\", (len(images)*w, h))\n",
        "    for i, img_ in enumerate(images):\n",
        "        img.paste(img_, (i*w,0))\n",
        "    return img\n",
        "\n",
        "p_generate = jax.pmap(generate, \"batch\")\n",
        "p_get_images = jax.pmap(get_images, \"batch\")\n",
        "\n",
        "bart_params = replicate(model.params)\n",
        "vqgan_params = replicate(vqgan.params)\n",
        "\n",
        "clear_output()"
      ],
      "id": "6eb74941-bb4d-4d7e-97f1-d5a3a07672bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dD0m3KuGxnf8"
      },
      "source": [
        "#@title # **Inferencing** try it out!\n",
        "\n",
        "input = \"white snow covered mountain under blue sky during daytime\" #@param {type:\"string\"}\n",
        "\n",
        "prompts = [input]\n",
        "\n",
        "\n",
        "prompt = [prompts[0]] * jax.device_count()\n",
        "inputs = tokenizer(prompt, return_tensors='jax', padding=\"max_length\", truncation=True, max_length=128).data\n",
        "inputs = shard(inputs)\n",
        "\n",
        "for i in range(4):\n",
        "    key = random.randint(0, 1e7)\n",
        "    rng = jax.random.PRNGKey(key)\n",
        "    rngs = jax.random.split(rng, jax.local_device_count())\n",
        "    indices = p_generate(inputs, rngs, bart_params).sequences\n",
        "    indices = indices[:, :, 1:]\n",
        "\n",
        "    images = p_get_images(indices, vqgan_params)\n",
        "    images = np.squeeze(np.asarray(images), 1)\n",
        "    imges = [custom_to_pil(image) for image in images]\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.imshow(stack_reconstructions(imges))"
      ],
      "id": "dD0m3KuGxnf8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6e1060f"
      },
      "source": [
        "## **CLIP Scoring** Optional"
      ],
      "id": "b6e1060f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c68724bc",
        "cellView": "form"
      },
      "source": [
        "#@title # **Setup CLIP scoring** run this once.\n",
        "\n",
        "from transformers import CLIPProcessor, FlaxCLIPModel\n",
        "\n",
        "clip = FlaxCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def hallucinate(prompt, num_images=64):\n",
        "    prompt = [prompt] * jax.device_count()\n",
        "    inputs = tokenizer(prompt, return_tensors='jax', padding=\"max_length\", truncation=True, max_length=128).data\n",
        "    inputs = shard(inputs)\n",
        "\n",
        "    all_images = []\n",
        "    for i in range(num_images // jax.device_count()):\n",
        "        key = random.randint(0, 1e7)\n",
        "        rng = jax.random.PRNGKey(key)\n",
        "        rngs = jax.random.split(rng, jax.local_device_count())\n",
        "        indices = p_generate(inputs, rngs, bart_params).sequences\n",
        "        indices = indices[:, :, 1:]\n",
        "\n",
        "        images = p_get_images(indices, vqgan_params)\n",
        "        images = np.squeeze(np.asarray(images), 1)\n",
        "        for image in images:\n",
        "            all_images.append(custom_to_pil(image))\n",
        "    return all_images\n",
        "\n",
        "def clip_top_k(prompt, images, k=8):\n",
        "    inputs = processor(text=prompt, images=images, return_tensors=\"np\", padding=True)\n",
        "    outputs = clip(**inputs)\n",
        "    logits = outputs.logits_per_text\n",
        "    scores = np.array(logits[0]).argsort()[-k:][::-1]\n",
        "    return [images[score] for score in scores]\n",
        "\n",
        "clear_output()"
      ],
      "id": "c68724bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00605e13",
        "cellView": "form"
      },
      "source": [
        "#@title # **Inferencing with CLIP scoring**\n",
        "#@markdown ### The best images will appear first.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "prompt = \"white snow covered mountain under blue sky during daytime\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown #### Amount of images to generate. Usually more the better.\n",
        "\n",
        "total_images = 64 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown #### Highest rated images from CLIP to show after generating finished.\n",
        "\n",
        "show_top_results = 8 #@param {type:\"integer\"}\n",
        "\n",
        "images = hallucinate(prompt, total_images)\n",
        "selected = clip_top_k(prompt, images, k=show_top_results)\n",
        "stack_reconstructions(selected)"
      ],
      "id": "00605e13",
      "execution_count": null,
      "outputs": []
    }
  ]
}
